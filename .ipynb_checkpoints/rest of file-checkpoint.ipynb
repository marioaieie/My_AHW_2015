{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian SN Cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rubin et al. 2015](http://arxiv.org/abs/1507.01602) lays out a pretty complete Bayesian model for going from light curve parameters for a large, heterogeneous set of SNe to inferred cosmological parameters. However, it has a couple deficiencies: (1) There's no publicly available code, meaning that the model cannot be modified and improved. (2) The implementation runs slowly (timescale of days). In this hack, we'll start on an open-source package implementing the model and seeing if we can take advantage of aspects of the problem to make it run a whole lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Simplified SN Cosmology PGM.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mostly just following the Rubin et al. 2015 explanation, quoted below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import sncosmo\n",
    "import matplotlib.pylab as plt\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Each simulated dataset has 250 SNe, except the highest-redshift, which has 50.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_we_want = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We generate the $x_{1}$ population from a unit normal distribution, centered on zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_true_dist = np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We assume that the uncertainties on $m_B$, $x_{1}$, and $c$ are 0.05, 0.5, and 0.05, and are uncorrelated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_obs_unc = 0.5*np.ones_like(x1_true_dist)\n",
    "\n",
    "x1_obs_dist = x1_true_dist + x1_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We draw the population $c$ values from the sum of a Gaussian distribution of width 0.1 magnitudes and an exponential with rate 1/(0.1 magnitudes). We center the distribution on zero.\" $\\beta$ is the scale parameter, which is the inverse of the rate parameter $\\lambda = 1/\\beta$.  The rate parameter is an alternative, widely used parameterization of the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_true_norm = np.random.normal(loc=0, scale=0.1, size=size_we_want)\n",
    "c_true_exp = np.random.exponential(scale=0.1/1, size=size_we_want)\n",
    "c_true_dist = np.add(c_true_norm, c_true_exp)\n",
    "\n",
    "c_obs_unc = 0.5*np.ones_like(c_true_dist)\n",
    "\n",
    "c_obs_dist = c_true_dist + c_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We generate four simulated datasets spanning the redshift ranges 0.02-0.05, 0.05-0.4, 0.2-1.0, and 0.7-1.4.\" We're just doing one set for now, from 0.2-1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_dist = np.random.uniform(0.2, 1.0, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ and $\\beta$ are assumed to be constant, with values 0.13 and 3.0, respectively. $M_{B}$ is set to -19.1 and $\\Omega_{m}$ is set to 0.3 (flat $\\Lambda \\text{CDM}$ model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.13\n",
    "beta = 3.0\n",
    "MB = -19.1\n",
    "Omega_m = 0.3\n",
    "sigma_int = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated the $\\mu$ values using astropy.cosmology.lambdacdm()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosmo = FlatLambdaCDM(H0=70, Om0=Omega_m)\n",
    "mu_dist = cosmo.distmod(z_dist).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We assume that the intrinsic dispersion covariance matrix is correct in SALT2, and that only dispersion in $m_{B}$ (gray dispersion) remains. The statistical model does not have access to this information, and fits for the full unknown matrix, overestimating the uncertainties on $x_{1}$ and $c$, and thus slightly biasing $\\alpha$ and $\\beta$ away from zero (see Section 2.5). (This is not a unique problem for our framework; the old technique would have the same bias.)\" We might come back to this but for now we just solved for $m_{B} = M_{B} - {\\alpha}x_{1} + {\\beta}{c} + \\mu$ and introduced some scatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3c4d3b4babf5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3c4d3b4babf5>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    mb_true =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mb_true_mean = MB - alpha*x1_true_dist + beta*c_true_dist + mu_dist \n",
    "mb_true_var = sigma_int**2\n",
    "\n",
    "mb_true = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(mb_true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simple Monte Carlo Inference\n",
    "\n",
    "If we were making mock data, we would do the following:\n",
    "\n",
    "```python\n",
    "    mb_obs_unc = 0.05*np.ones_like(mb_true_dist)\n",
    "\n",
    "    mb_obs_dist = mb_true_dist + mb_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)\n",
    "```\n",
    "\n",
    "However, right now we're going to get set up to do inference by Simple Monte Carlo: weight each sample by its parameters likelihood, and approximate posterior integrals with likelihood-weighted sums over prior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's package up the parameters, ML-style:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#par_labels = ['x1_true', 'x1_obs', 'c_true', 'c_obs', 'mb_true', 'mb_obs', 'alpha', 'beta', 'Mb', 'Omega', 'z', 'x1_dist', 'c_dist', 'sigma_int']\n",
    "par_labels = ['x1_true', 'c_true', 'mb_true', 'alpha', 'beta', 'MB', 'Omega_m', 'sigma_int']\n",
    "\n",
    "#pd.Series([1,3,5], index=['x1_true']) for i in nrange\n",
    "#x1_obs = pd.Series([2,4,6], index=['x1_obs'])\n",
    "\n",
    "#SN = [ pd.Series(np.ones(len(par_labels)),index=par_labels) for i in range(4)]\n",
    "#print parameters\n",
    "\n",
    "#datafr = pd.DataFrame(SN)\n",
    "datafr = pd.DataFrame(np.ones((10,len(par_labels))),columns=par_labels)\n",
    "datafr\n",
    "#SN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "nSN = 10\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=Omega_m)\n",
    "mu = cosmo.distmod(z).value\n",
    "\n",
    "def LogGaussian(x,mu,sig):\n",
    "    \n",
    "    return -0.5*(x-mu)**2 /sig**2\n",
    "\n",
    "schain = []\n",
    "for i in range(N):\n",
    "    datafr = pd.DataFrame(np.ones((nSN,len(par_labels))),columns=par_labels)\n",
    "    datafr['alpha'] = scipy.stats.norm.rvs()\n",
    "    datafr['beta'] = scipy.stats.norm.rvs()\n",
    "    datafr['MB'] = scipy.stats.norm.rvs()\n",
    "    datafr['sigma_int'] = scipy.stats.norm.rvs()\n",
    "    datafr['Omega_m'] = scipy.stats.norm.rvs()\n",
    "    datafr['x1_true'] = scipy.stats.norm.rvs(size=nSN)\n",
    "    datafr['c_true'] = scipy.stats.norm.rvs(size=nSN)\n",
    "\n",
    "    mb_true = datafr['MB'] - datafr['alpha']*datafr['x1_true'] + datafr['beta']*datafr['c_true'] + mu\n",
    "    likelihood_mobs = LogGaussian(mb_obs, mb_true, datafr['sigma_int']**2)\n",
    "\n",
    "    likelihood_x1 = LogGaussian(x1_obs, datafr['x1_true'], 0.5**2)\n",
    "\n",
    "    likelihood_c = LogGaussian(c_obs, datafr['c_true'], 0.5**2)\n",
    "\n",
    "    likelihood = np.sum([likelihood_mobs, likelihood_x1, likelihood_c])\n",
    "    \n",
    "    schain.append(datafr[['mb_true', 'alpha', 'beta', 'MB', 'Omega_m', 'sigma_int']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schain = []\n",
    "schain.append(datafr[['mb_true', 'alpha', 'beta', 'MB', 'Omega_m', 'sigma_int']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = dict(par_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "datafr['alpha'] = scipy.stats.norm.rvs()\n",
    "datafr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafr = datafr.apply(lambda x: np.random.normal(size=x.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(parameters):\n",
    "    L = 1.0 # for now...\n",
    "    return L    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_by_likelihood(parameters):\n",
    "    #  Set likelihood column of parameters:\n",
    "    parameters['weight'] = likelihood(parameters)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_by_likelihood(datafr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(datafr['Omega'],histtype='stepfilled',normed=True)\n",
    "plt.hist(np.random.normal(size=1e4),histtype='step',color='r',normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
