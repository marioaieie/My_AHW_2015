{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bayesian SN Cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Rubin et al. 2015](http://arxiv.org/abs/1507.01602) lays out a pretty complete Bayesian model for going from light curve parameters for a large, heterogeneous set of SNe to inferred cosmological parameters. However, it has a couple deficiencies: (1) There's no publicly available code, meaning that the model cannot be modified and improved. (2) The implementation runs slowly (timescale of days). In this hack, we'll start on an open-source package implementing the model and seeing if we can take advantage of aspects of the problem to make it run a whole lot faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Simplistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Simplified SN Cosmology PGM.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assigning Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mostly just following the Rubin et al. 2015 explanation, quoted below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import sncosmo\n",
    "#import matplotlib as plt\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Each simulated dataset has 250 SNe, except the highest-redshift, which has 50.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_we_want = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We generate the $x_{1}$ population from a unit normal distribution, centered on zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_true_dist = np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We assume that the uncertainties on $m_B$, $x_{1}$, and $c$ are 0.05, 0.5, and 0.05, and are uncorrelated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_obs_unc = 0.5*np.ones_like(x1_true_dist)\n",
    "\n",
    "x1_obs_dist = x1_true_dist + x1_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We draw the population $c$ values from the sum of a Gaussian distribution of width 0.1 magnitudes and an exponential with rate 1/(0.1 magnitudes). We center the distribution on zero.\" $\\beta$ is the scale parameter, which is the inverse of the rate parameter $\\lambda = 1/\\beta$.  The rate parameter is an alternative, widely used parameterization of the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_true_norm = np.random.normal(loc=0, scale=0.1, size=size_we_want)\n",
    "c_true_exp = np.random.exponential(scale=0.1/1, size=size_we_want)\n",
    "c_true_dist = np.add(c_true_norm, c_true_exp)\n",
    "\n",
    "c_obs_unc = 0.5*np.ones_like(c_true_dist)\n",
    "\n",
    "c_obs_dist = c_true_dist + c_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We generate four simulated datasets spanning the redshift ranges 0.02-0.05, 0.05-0.4, 0.2-1.0, and 0.7-1.4.\" We're just doing one set for now, from 0.2-1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_dist = np.random.uniform(0.2, 1.0, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ and $\\beta$ are assumed to be constant, with values 0.13 and 3.0, respectively. $M_{B}$ is set to -19.1 and $\\Omega_{m}$ is set to 0.3 (flat $\\Lambda \\text{CDM}$ model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.13\n",
    "beta = 3.0\n",
    "MB = -19.1\n",
    "Omega_m = 0.3\n",
    "sigma_int = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated the $\\mu$ values using astropy.cosmology.lambdacdm()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosmo = FlatLambdaCDM(H0=70, Om0=Omega_m)\n",
    "mu_dist = cosmo.distmod(z_dist).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We assume that the intrinsic dispersion covariance matrix is correct in SALT2, and that only dispersion in $m_{B}$ (gray dispersion) remains. The statistical model does not have access to this information, and fits for the full unknown matrix, overestimating the uncertainties on $x_{1}$ and $c$, and thus slightly biasing $\\alpha$ and $\\beta$ away from zero (see Section 2.5). (This is not a unique problem for our framework; the old technique would have the same bias.)\" We might come back to this but for now we just solved for $m_{B} = M_{B} - {\\alpha}x_{1} + {\\beta}{c} + \\mu$ and introduced some scatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_true_dist = MB - alpha*x1_true_dist + beta*c_true_dist + mu_dist + np.random.normal(loc=0, scale=sigma_int, size=size_we_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simple Monte Carlo Inference\n",
    "\n",
    "If we were making mock data, we would do the following:\n",
    "\n",
    "```python\n",
    "    mb_obs_unc = 0.05*np.ones_like(mb_true_dist)\n",
    "\n",
    "    mb_obs_dist = mb_true_dist + mb_obs_unc*np.random.normal(loc=0, scale=1, size=size_we_want)\n",
    "```\n",
    "\n",
    "However, right now we're going to get set up to do inference by Simple Monte Carlo: weight each sample by its parameters likelihood, and approximate posterior integrals with likelihood-weighted sums over prior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_true      1\n",
      "x1_obs       1\n",
      "c_true       1\n",
      "c_obs        1\n",
      "mb_true      1\n",
      "mb_obs       1\n",
      "alpha        1\n",
      "beta         1\n",
      "Mb           1\n",
      "Omega        1\n",
      "z            1\n",
      "x1_dist      1\n",
      "c_dist       1\n",
      "sigma_int    1\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Let's package up the parameters, ML-style:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "par_labels = ['x1_true', 'x1_obs', 'c_true', 'c_obs', 'mb_true', 'mb_obs', 'alpha', 'beta', 'Mb', 'Omega', 'z', 'x1_dist', 'c_dist', 'sigma_int']\n",
    "\n",
    "#x1_true = pd.Series([1,3,5], index=['x1_true'])\n",
    "#x1_obs = pd.Series([2,4,6], index=['x1_obs'])\n",
    "\n",
    "parameters = pd.Series(np.ones(len(par_labels)), index=par_labels)\n",
    "print parameters\n",
    "\n",
    "#datafr = pd.DataFrame(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(parameters):\n",
    "    L = 1.0 # for now...\n",
    "    return L    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_by_likelihood(parameters):\n",
    "    #  Set likelihood column of parameters:\n",
    "    parameters['weight'] = likelihood(parameters)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
